{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import statistics as stats\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')\n",
    "import sys\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"news_sample.csv\")#[:10]\n",
    "df.drop_duplicates(subset='content', inplace=True,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipfsFiltering(df,quantiles=[0.05,0.95],generateGraph=True):\n",
    "  #df[\"content\"] = df[\"content\"].lower()\n",
    "  for y in [\"content\"]:\n",
    "    tokens = nltk.tokenize.word_tokenize(df[y])\n",
    "    allWordsDist = nltk.FreqDist(w.lower() for w in tokens)\n",
    "\n",
    "    words = [[word,dict(allWordsDist.most_common())[word]] for word in dict(allWordsDist.most_common()) if word.isalpha()]\n",
    "    words = sorted(words,key=lambda k: k[1],reverse=True)\n",
    "      #print(words)\n",
    "\n",
    "    wordCount = [x[1] for x in words]\n",
    "    lower = int(np.percentile(wordCount,100*(quantiles[0])))\n",
    "    upper = int(np.percentile(wordCount,100*(quantiles[1])))\n",
    "    \n",
    "    for word in words:\n",
    "      if word[1] >= upper:\n",
    "        df[y] = df[y].replace(f\" {word[0]} \",\" \")\n",
    "        words.remove(word)\n",
    "      elif word[1] <= lower:\n",
    "        df[y] = df[y].replace(f\" {word[0]} \",\" \")\n",
    "        words.remove(word)\n",
    "    \n",
    "      \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(df[\"content\"])\n",
    "    df[\"content\"] = ' '.join([word for word in tokens if not word in stop_words])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywordFiltering(df):\n",
    "    for y in [\"content\"]:\n",
    "        try:\n",
    "            df[y] = str(df[y]).lower() #make lowercase\n",
    "            df[y] = re.sub(r\"\\t\",\" \",str(df[y])) #Remove tab\n",
    "            df[y] = re.sub(r\"\\n\",\" \",str(df[y])) #Remove newline\n",
    "            \n",
    "            #df[y] = re.sub(r\"\\bhttp.*[^ ]\",\"<URL>\",str(df[y])) #Remove Url\n",
    "            #df[y] = re.sub(r\"www\\..+?\",\"<URL> \",str(df[y]))\n",
    "            #df[y] = re.sub(r\"\\b.*\\.com.*\\b\",\"<URL>\",str(df[y]))  \n",
    "            df[y] = re.sub(r\"((http://|https://)*(www\\.)*([\\w\\d\\._-]+)(\\.[\\w]{2,})(\\.)*?(/[\\w\\d#%=&/?\\.+_-]+)*(\\.[\\w]+)*)\",\n",
    "                                \"<URL>\", str(df[y]))              \n",
    "            \n",
    "            df[y] = re.sub(r\"\\d{4}[-|\\/|\\\\]\\d{2}[-|\\/|\\\\]\\d{2}\\b\",\"<DATE>\",str(df[y])) #Remove Date\n",
    "            df[y] = re.sub(r\"\\b\\d{2}[-|\\/|\\\\]{1}\\d{2}[-|\\/|\\\\]{1}\\d{2}\\b\",\"<DATE>\",str(df[y])) \n",
    "            df[y] = re.sub(r\"\\b\\d{2}[-|\\/|\\\\]{1}\\d{2}[-|\\/|\\\\]{1}\\d{4}\\b\",\"<DATE>\",str(df[y]))\n",
    "            df[y] = re.sub(r\"((jan[uary]*|feb[ruary]*|mar[ch]*|apr[il]*|may|jun[e]*|jul[y]*|aug[ust]*|sep[tember]*|oct[ober]*|nov[ember]*|dec[ember]*) ([\\d]+(\\w{2})*) ?(rd|st|th+))\",\n",
    "                                \"<DATE>\", str(df[y]))\n",
    "            df[y] = re.sub(r\"\\d{1,2}?(rd|st|th)\", \"<DATE>\", str(df[y])) #match format num(th, rd, st)\n",
    "                \n",
    "            df[y] = re.sub(r\"\\b[\\w\\.\\-]+[\\d\\w]+?[@][\\w]+?[\\.][a-z]{2,}\\b\", \"<EMAIL>\", str(df[y])) #Remove email \n",
    "            \n",
    "            #df[y] = re.sub(r\".+@.+\",\"<Twitter>\",str(df[y])) #Can remove twitter \n",
    "            \n",
    "            df[y] = re.sub(r\"[0-9]+[\\.|,|:|0-9]*\",\"<NUM>\",str(df[y])) #Remove num\n",
    "\n",
    "            df[y] = re.sub(r\"[^\\s\\w\\d]\", \"\", str(df[y])) #remove punctuation\n",
    "            df[y] = re.sub(r\" {2,}\",\" \",str(df[y])) #Remove extra white space\n",
    "\n",
    "        except:\n",
    "            if not isinstance(df[\"content\"],str):\n",
    "                df[y] = \"\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyStemming(df):\n",
    "    tokens = word_tokenize(df[\"content\"])\n",
    "    tokenTags = nltk.tag.pos_tag(tokens)\n",
    "    ret = []\n",
    "    for (x,y) in tokenTags:\n",
    "        if \"VB\" in y:\n",
    "            ret.append(PorterStemmer().stem(x))\n",
    "        else:\n",
    "            ret.append(WordNetLemmatizer().lemmatize(x))\n",
    "    df[\"content\"] = ' '.join(ret)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploringData(df):\n",
    "    propNounsFake = 0\n",
    "    fakeTotal = 0\n",
    "\n",
    "    propNounsElse = 0\n",
    "    elseTotal = 0\n",
    "\n",
    "\n",
    "    for y in [\"content\"]:\n",
    "        for x in range(0,len(df)):\n",
    "            try:\n",
    "                if df[\"type\"][x].lower() == 'fake':\n",
    "                    fakeTotal +=1\n",
    "                    sentences = sent_tokenize(df[y][x])\n",
    "                    words = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "                    words = words[0]\n",
    "                    print(words)\n",
    "\n",
    "                    tagged_words = [nltk.pos_tag(sentence) for sentence in words]\n",
    "                    print(f\"tags: {tagged_words}\")\n",
    "                    proper_nouns = []\n",
    "                    for sentence in tagged_words:\n",
    "                        for word, tag in sentence:\n",
    "                            if tag == 'NNP': # NNP denotes proper noun\n",
    "                                proper_nouns.append(word)\n",
    "                    print(proper_nouns)\n",
    "                    propNounsFake += len(set(proper_nouns))\n",
    "\n",
    "\n",
    "                else:\n",
    "                    elseTotal +=1\n",
    "                    sentences = sent_tokenize(df[y][x])\n",
    "                    words = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "                    words = words[0]\n",
    "                    print(words)\n",
    "\n",
    "                    tagged_words = [nltk.pos_tag(sentence) for sentence in words]\n",
    "                    print(f\"tags: {tagged_words}\")\n",
    "                    proper_nouns = []\n",
    "                    for sentence in tagged_words:\n",
    "                        for word, tag in sentence:\n",
    "                            if tag == 'NNP': # NNP denotes proper noun\n",
    "                                proper_nouns.append(word)\n",
    "                    print(proper_nouns)\n",
    "                    propNounsElse += len(set(proper_nouns))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    print(f\"Prop nouns fake {propNounsFake/fakeTotal} else: {propNounsElse/elseTotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueGraph(df):\n",
    "    fakeArticles = []\n",
    "    reliableArticles = []\n",
    "\n",
    "    fakeWords = []\n",
    "    reliableWords = []\n",
    "\n",
    "    #for y in df[\"content\"]:\n",
    "    for x in range(0, len(df)):\n",
    "        if df[\"type\"][x].lower() == \"fake\":\n",
    "            fakeWords = Counter(sorted(word_tokenize(df[\"content\"][x])))\n",
    "            fakeArticles.append(len(fakeWords))\n",
    "        else:\n",
    "            reliableWords = Counter(sorted(word_tokenize(df[\"content\"][x])))\n",
    "            reliableArticles.append(len(reliableWords))\n",
    "\n",
    "    AvFake = sum(fakeArticles)/len(fakeArticles)\n",
    "    AvReliable = sum(reliableArticles)/len(reliableArticles)\n",
    "    dif = (AvFake-AvReliable)/AvFake*100\n",
    "\n",
    "    print(\"Unique words in fake articles: \" + str(AvFake))\n",
    "    print(\"Unique words in reliable articles: \" + str(AvReliable))\n",
    "    print(\"Difference: {} %\".format(math.floor(dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fakenessFromWord(df, word):\n",
    "\n",
    "    fakeWord = 0\n",
    "    reliableWord = 0\n",
    "\n",
    "    fakeNoWord = 0\n",
    "    reliableNoWord = 0\n",
    "\n",
    "    word = word.lower()\n",
    "\n",
    "\n",
    "    for x in range(0, len(df)):\n",
    "        result = re.search(rf\"\\b{word}\\b\", str(df[\"content\"][x]))\n",
    "        try:\n",
    "            if not(result == None): #the word is found\n",
    "                if df[\"type\"][x].lower() == \"fake\":\n",
    "                    fakeWord += 1\n",
    "                else:\n",
    "                    reliableWord += 1\n",
    "            else:\n",
    "                if df[\"type\"][x].lower() == \"fake\":\n",
    "                    fakeNoWord += 1\n",
    "                else:\n",
    "                    reliableNoWord += 1\n",
    "        except:\n",
    "            pass\n",
    "    print(\"fakeword: {}\\n reliableword: {}\\n fakenoword: {} \\n reliableNoword: {}\".format(fakeWord, reliableWord, fakeNoWord, reliableNoWord))\n",
    "    print(df.shape)\n",
    "    #percentage of fake articles with the word out of all fake articles\n",
    "    preFake = (1 - (fakeNoWord/(fakeNoWord + fakeWord)))*100\n",
    "    print(\"Percentage of fake articles with the word: {}%\".format(preFake))\n",
    "\n",
    "    #percentage of reliable articles with the word out of all reliabel articles\n",
    "    preReliable = (1 - (reliableNoWord/(reliableNoWord + reliableWord)))*100\n",
    "    print(\"Percentage of reliable articles with the word: {}%\".format(preReliable))\n",
    "\n",
    "    #out of all articles with the word X% of them are fake\n",
    "    fakeWordCorrelation = (fakeWord/(fakeWord + reliableWord))*100\n",
    "    print(fakeWordCorrelation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclamationFunction(df):\n",
    "\n",
    "    fakeExclamations = []\n",
    "    nonFakeExclamations = []\n",
    "\n",
    "    fakeNoExclamations = 0\n",
    "    nonFakeNoExclamations = 0\n",
    "    k = 0\n",
    "\n",
    "    for x in range(0, len(df)):\n",
    "        if str(df['type'][x]).lower() == 'fake':\n",
    "            # counting number of fake articles\n",
    "            k += 1\n",
    "        excl = re.findall('!', df['content'][x])\n",
    "        if len(excl) == 0: \n",
    "            if str(df['type'][x]).lower() == 'fake':\n",
    "                fakeNoExclamations += 1\n",
    "            else:\n",
    "                nonFakeNoExclamations += 1\n",
    "        else:\n",
    "            if str(df['type'][x]).lower() == 'fake':\n",
    "                fakeExclamations.append(len(excl))\n",
    "            else:\n",
    "                nonFakeExclamations.append(len(excl))\n",
    "\n",
    "    # mean number of exclamation marks in fake and not fake articles\n",
    "    fakeExclMean = stats.mean(fakeExclamations)\n",
    "    nonFakeExclMean = stats.mean(nonFakeExclamations)\n",
    "\n",
    "    # percentage of fake articles without exclamations\n",
    "    fNE = (fakeNoExclamations / k)*100\n",
    "\n",
    "    print(\"If the article is fake and has exclamation marks, there are on average {} of them\".format(fakeExclMean))\n",
    "    print(\"If the article isn't fake, and has exclamation marks, there are on average {} of them\".format(nonFakeExclMean))\n",
    "\n",
    "    print(\"Of the {} total articles, {} of them are fake\".format(len(df), k))\n",
    "    print(\"Of the {} fake articles, {} don't have exclamation marks in - {}%\".format(k, fakeNoExclamations, fNE))\n",
    "\n",
    "    # if fake, percentage chance of exclamation\n",
    "    ifFake = ((k/len(df)) * (len(fakeExclamations)/k))/(\n",
    "                (k/len(df)) * (len(fakeExclamations)/k) + (\n",
    "                (1 - (k/len(df))) * (1 - (len(fakeExclamations)/k)))) * 100\n",
    "\n",
    "    # if not fake, perecentage chance of exclamation\n",
    "    ifNonFake = (((len(df) - k)/len(df)) * (len(nonFakeExclamations)/(\n",
    "                len(df) - k))) / ((((len(df) - k)/len(df)) * (\n",
    "                len(nonFakeExclamations)/(len(df) - k))) + ((1 - (\n",
    "                (len(df) - k)/len(df))) * (1 - (len(nonFakeExclamations)/(\n",
    "                len(df) - k))))) * 100\n",
    "\n",
    "    print(\"If an article is fake, there is a {}% chance that it has exclamation marks\".format(ifFake))\n",
    "    print(\"If an article isn't fake, there is a {}% chance that it has exclamation marks\".format(ifNonFake))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing types to a binary classification\n",
    "# Reliable is either with type reliable, clickbait or political - becomes 1\n",
    "# Fake is the rest - becomes 0\n",
    "\n",
    "def classifierRelOrFake(df):\n",
    "    for x in range(0, len(df)):\n",
    "        if df['type'][x] == 'reliable' or df['type'][x] == 'clickbait' or df['type'][x] == 'political':\n",
    "            df['type'][x] = 1 \n",
    "        else:\n",
    "            df['type'][x] = 0 \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in range(len(df)):\n",
    "#     print(\"Authors:\",df['authors'][x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the article is fake and has exclamation marks, there are on average 3.114503816793893 of them\n",
      "If the article isn't fake, and has exclamation marks, there are on average 2.388888888888889 of them\n",
      "Of the 239 total articles, 146 of them are fake\n",
      "Of the 146 fake articles, 15 don't have exclamation marks in - 10.273972602739725%\n",
      "If an article is fake, there is a 93.20208566833975% chance that it has exclamation marks\n",
      "If an article isn't fake, there is a 13.260456273764259% chance that it has exclamation marks\n"
     ]
    }
   ],
   "source": [
    "# for x in range(0,len(df)):\n",
    "#     df.iloc[x] = zipfsLaw.zipfsFiltering(df.iloc[x])\n",
    "#     df.iloc[x] = removeStopwords(df.iloc[x])\n",
    "#     df.iloc[x] = keywordFiltering(df.iloc[x])\n",
    "#     df.iloc[x] = stemming.applyStemming(df.iloc[x])\n",
    "\n",
    "    #tilføj funktioner husk kun at give en linje\n",
    "# fakenessFromWord(df, \"trump\")\n",
    "exclamationFunction(df)\n",
    "classifierRelOrFake(df)\n",
    "# uniqueGraph(df)\n",
    "\n",
    "# exploringData(df)\n",
    "# print(df)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['content'], df['type'],\n",
    "                                                    test_size=0.2,random_state=0)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, train_size=0.5,\n",
    "                                                random_state=0)\n",
    "# lr = LinearRegression()\n",
    "# model = lr.fit(X_train, y_train)\n",
    "# print(model.score(X_train, y_train))\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "df.to_csv(\"Results.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Shopping: Yule Gifts for Witches and Wizards\\r\\n\\r\\n% of readers think this story is Fact. Add your two cents.\\r\\n\\r\\nHeadline: Bitcoin & Blockchain Searches Exceed Trump! Blockchain Stocks Are Next!\\r\\n\\r\\nWhen I was in Treadwell’s occult bookshop in London earlier this week I photographed the display of things that would really make wonderful gifts for anyone interested in witchcraft and magic.\\r\\n\\r\\nI was particularly taken with the shops exclusive gift boxes for witches and magicians. You can see the Witch’s Gift Box to the left and in the picture above, nestled in among a whole range of books that would also be great witchy gifts. (And yes, I was pretty chuffed to see my own book, Pagan Portals – Candle Magic, in the front row.)\\r\\n\\r\\nBut here is what Treadwell’s has to say about the Witch’s Gift Box:\\r\\n\\r\\nFor that special Witch in your life we have the perfect gift, a box full of delights. We have selected what we think are all the things a Witch needs, perhaps most important of all is a great big mug for cups of tea while they contemplate the mysteries of the universe.\\r\\n\\r\\nThe Witch’s Gift Box includes: A pendulum, Rider Waite tarot deck, a small crystal ball, a hare pentacle, a witch doll, a bag of cone incense, a book on herbal healing, 4 quarter candles, a Witch’s scroll and Witch’s brew mug. All carefully packed in a beautiful box. Price: £50.\\r\\n\\r\\nMind you, although that really appealed to me, I do already have everything it contains, so I’m not personally dropping hints. However, if anyone was wanting to spend a bit of money on me for Yule, I don’t actually own either of the main items in Treadwell’s Crowley Magick Box: The Book of the Law and the Thoth Tarot.\\r\\n\\r\\n\\r\\n\\r\\nHere’s the description in full:\\r\\n\\r\\nCrowley Magick Box: A box to start you on your way with the mysteries of Magick. The essential Book of the Law, with black and while pillar candles, incense, oil, ink, the beautiful Thoth tarot deck, a Baphomet scroll and mug. All packed in a beautiful box, perfect to store your growing collection of magical tools. Price: £50.\\r\\n\\r\\nTreadwell’s Bookshop is at 33 Store Street, London, WC1E 7BS. Tel: 020 7419 8507. Email: info@treadwells-london.com.\\r\\n\\r\\nLinks\\r\\n\\r\\nhttps://www.treadwells-london.com/\\r\\n\\r\\nPagan Portals – Candle Magic\\r\\n\\r\\nWitch’s Gift Box\\r\\n\\r\\nCrowley Magick Box\\r\\n\\r\\nTo read more posts like this visit A Bad Witch’s Blog at www.badwitch.co.uk\\r\\n\\r\\nSource: http://www.badwitch.co.uk/2017/12/shopping-yule-gifts-for-witches-and.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1908/3418560743.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnaive_bayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1908/3418560743.py\u001b[0m in \u001b[0;36mnaive_bayes\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmultiNB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmultiNB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mmultiNB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultiNB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jacob\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    747\u001b[0m         \"\"\"\n\u001b[0;32m    748\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 749\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    750\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jacob\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[1;34m(self, X, y, reset)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m         \u001b[1;34m\"\"\"Validate X and y in fit methods.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_class_log_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jacob\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jacob\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1104\u001b[0m         )\n\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jacob\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    880\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m                 raise ValueError(\n",
      "\u001b[1;32mc:\\Users\\jacob\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"numpy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"numpy.array_api\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jacob\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Shopping: Yule Gifts for Witches and Wizards\\r\\n\\r\\n% of readers think this story is Fact. Add your two cents.\\r\\n\\r\\nHeadline: Bitcoin & Blockchain Searches Exceed Trump! Blockchain Stocks Are Next!\\r\\n\\r\\nWhen I was in Treadwell’s occult bookshop in London earlier this week I photographed the display of things that would really make wonderful gifts for anyone interested in witchcraft and magic.\\r\\n\\r\\nI was particularly taken with the shops exclusive gift boxes for witches and magicians. You can see the Witch’s Gift Box to the left and in the picture above, nestled in among a whole range of books that would also be great witchy gifts. (And yes, I was pretty chuffed to see my own book, Pagan Portals – Candle Magic, in the front row.)\\r\\n\\r\\nBut here is what Treadwell’s has to say about the Witch’s Gift Box:\\r\\n\\r\\nFor that special Witch in your life we have the perfect gift, a box full of delights. We have selected what we think are all the things a Witch needs, perhaps most important of all is a great big mug for cups of tea while they contemplate the mysteries of the universe.\\r\\n\\r\\nThe Witch’s Gift Box includes: A pendulum, Rider Waite tarot deck, a small crystal ball, a hare pentacle, a witch doll, a bag of cone incense, a book on herbal healing, 4 quarter candles, a Witch’s scroll and Witch’s brew mug. All carefully packed in a beautiful box. Price: £50.\\r\\n\\r\\nMind you, although that really appealed to me, I do already have everything it contains, so I’m not personally dropping hints. However, if anyone was wanting to spend a bit of money on me for Yule, I don’t actually own either of the main items in Treadwell’s Crowley Magick Box: The Book of the Law and the Thoth Tarot.\\r\\n\\r\\n\\r\\n\\r\\nHere’s the description in full:\\r\\n\\r\\nCrowley Magick Box: A box to start you on your way with the mysteries of Magick. The essential Book of the Law, with black and while pillar candles, incense, oil, ink, the beautiful Thoth tarot deck, a Baphomet scroll and mug. All packed in a beautiful box, perfect to store your growing collection of magical tools. Price: £50.\\r\\n\\r\\nTreadwell’s Bookshop is at 33 Store Street, London, WC1E 7BS. Tel: 020 7419 8507. Email: info@treadwells-london.com.\\r\\n\\r\\nLinks\\r\\n\\r\\nhttps://www.treadwells-london.com/\\r\\n\\r\\nPagan Portals – Candle Magic\\r\\n\\r\\nWitch’s Gift Box\\r\\n\\r\\nCrowley Magick Box\\r\\n\\r\\nTo read more posts like this visit A Bad Witch’s Blog at www.badwitch.co.uk\\r\\n\\r\\nSource: http://www.badwitch.co.uk/2017/12/shopping-yule-gifts-for-witches-and.html'"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def naive_bayes(df):\n",
    "\n",
    "    x = df['content'].to_numpy()\n",
    "    y = df['type'].to_numpy()\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "    multiNB = MultinomialNB()\n",
    "    multiNB.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = multiNB.predict(x_val)\n",
    "\n",
    "    nb_accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "    return nb_accuracy\n",
    "\n",
    "\n",
    "test = naive_bayes(df)\n",
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
