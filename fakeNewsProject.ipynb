{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import statistics as stats\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')\n",
    "import sys\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"news_sample.csv\")#[:10]\n",
    "df.drop_duplicates(subset='content', inplace=True,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipfsFiltering(df,quantiles=[0.05,0.95],generateGraph=True):\n",
    "  #df[\"content\"] = df[\"content\"].lower()\n",
    "  for y in [\"content\"]:\n",
    "    tokens = nltk.tokenize.word_tokenize(df[y])\n",
    "    allWordsDist = nltk.FreqDist(w.lower() for w in tokens)\n",
    "\n",
    "    words = [[word,dict(allWordsDist.most_common())[word]] for word in dict(allWordsDist.most_common()) if word.isalpha()]\n",
    "    words = sorted(words,key=lambda k: k[1],reverse=True)\n",
    "      #print(words)\n",
    "\n",
    "    wordCount = [x[1] for x in words]\n",
    "    lower = int(np.percentile(wordCount,100*(quantiles[0])))\n",
    "    upper = int(np.percentile(wordCount,100*(quantiles[1])))\n",
    "    \n",
    "    for word in words:\n",
    "      if word[1] >= upper:\n",
    "        df[y] = df[y].replace(f\" {word[0]} \",\" \")\n",
    "        words.remove(word)\n",
    "      elif word[1] <= lower:\n",
    "        df[y] = df[y].replace(f\" {word[0]} \",\" \")\n",
    "        words.remove(word)\n",
    "    \n",
    "      \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(df[\"content\"])\n",
    "    df[\"content\"] = ' '.join([word for word in tokens if not word in stop_words])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywordFiltering(df):\n",
    "    for y in [\"content\"]:\n",
    "        try:\n",
    "            df[y] = str(df[y]).lower() #make lowercase\n",
    "            df[y] = re.sub(r\"\\t\",\" \",str(df[y])) #Remove tab\n",
    "            df[y] = re.sub(r\"\\n\",\" \",str(df[y])) #Remove newline\n",
    "            \n",
    "            #df[y] = re.sub(r\"\\bhttp.*[^ ]\",\"<URL>\",str(df[y])) #Remove Url\n",
    "            #df[y] = re.sub(r\"www\\..+?\",\"<URL> \",str(df[y]))\n",
    "            #df[y] = re.sub(r\"\\b.*\\.com.*\\b\",\"<URL>\",str(df[y]))  \n",
    "            df[y] = re.sub(r\"((http://|https://)*(www\\.)*([\\w\\d\\._-]+)(\\.[\\w]{2,})(\\.)*?(/[\\w\\d#%=&/?\\.+_-]+)*(\\.[\\w]+)*)\",\n",
    "                                \"<URL>\", str(df[y]))              \n",
    "            \n",
    "            df[y] = re.sub(r\"\\d{4}[-|\\/|\\\\]\\d{2}[-|\\/|\\\\]\\d{2}\\b\",\"<DATE>\",str(df[y])) #Remove Date\n",
    "            df[y] = re.sub(r\"\\b\\d{2}[-|\\/|\\\\]{1}\\d{2}[-|\\/|\\\\]{1}\\d{2}\\b\",\"<DATE>\",str(df[y])) \n",
    "            df[y] = re.sub(r\"\\b\\d{2}[-|\\/|\\\\]{1}\\d{2}[-|\\/|\\\\]{1}\\d{4}\\b\",\"<DATE>\",str(df[y]))\n",
    "            df[y] = re.sub(r\"((jan[uary]*|feb[ruary]*|mar[ch]*|apr[il]*|may|jun[e]*|jul[y]*|aug[ust]*|sep[tember]*|oct[ober]*|nov[ember]*|dec[ember]*) ([\\d]+(\\w{2})*) ?(rd|st|th+))\",\n",
    "                                \"<DATE>\", str(df[y]))\n",
    "            df[y] = re.sub(r\"\\d{1,2}?(rd|st|th)\", \"<DATE>\", str(df[y])) #match format num(th, rd, st)\n",
    "                \n",
    "            df[y] = re.sub(r\"\\b[\\w\\.\\-]+[\\d\\w]+?[@][\\w]+?[\\.][a-z]{2,}\\b\", \"<EMAIL>\", str(df[y])) #Remove email \n",
    "            \n",
    "            #df[y] = re.sub(r\".+@.+\",\"<Twitter>\",str(df[y])) #Can remove twitter \n",
    "            \n",
    "            df[y] = re.sub(r\"[0-9]+[\\.|,|:|0-9]*\",\"<NUM>\",str(df[y])) #Remove num\n",
    "\n",
    "            df[y] = re.sub(r\"[^\\s\\w\\d]\", \"\", str(df[y])) #remove punctuation\n",
    "            df[y] = re.sub(r\" {2,}\",\" \",str(df[y])) #Remove extra white space\n",
    "\n",
    "        except:\n",
    "            if not isinstance(df[\"content\"],str):\n",
    "                df[y] = \"\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyStemming(df):\n",
    "    tokens = word_tokenize(df[\"content\"])\n",
    "    tokenTags = nltk.tag.pos_tag(tokens)\n",
    "    ret = []\n",
    "    for (x,y) in tokenTags:\n",
    "        if \"VB\" in y:\n",
    "            ret.append(PorterStemmer().stem(x))\n",
    "        else:\n",
    "            ret.append(WordNetLemmatizer().lemmatize(x))\n",
    "    df[\"content\"] = ' '.join(ret)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploringData(df):\n",
    "    propNounsFake = 0\n",
    "    fakeTotal = 0\n",
    "\n",
    "    propNounsElse = 0\n",
    "    elseTotal = 0\n",
    "\n",
    "\n",
    "    for y in [\"content\"]:\n",
    "        for x in range(0,len(df)):\n",
    "            try:\n",
    "                if df[\"type\"][x].lower() == 'fake':\n",
    "                    fakeTotal +=1\n",
    "                    sentences = sent_tokenize(df[y][x])\n",
    "                    words = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "                    words = words[0]\n",
    "                    print(words)\n",
    "\n",
    "                    tagged_words = [nltk.pos_tag(sentence) for sentence in words]\n",
    "                    print(f\"tags: {tagged_words}\")\n",
    "                    proper_nouns = []\n",
    "                    for sentence in tagged_words:\n",
    "                        for word, tag in sentence:\n",
    "                            if tag == 'NNP': # NNP denotes proper noun\n",
    "                                proper_nouns.append(word)\n",
    "                    print(proper_nouns)\n",
    "                    propNounsFake += len(set(proper_nouns))\n",
    "\n",
    "\n",
    "                else:\n",
    "                    elseTotal +=1\n",
    "                    sentences = sent_tokenize(df[y][x])\n",
    "                    words = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "                    words = words[0]\n",
    "                    print(words)\n",
    "\n",
    "                    tagged_words = [nltk.pos_tag(sentence) for sentence in words]\n",
    "                    print(f\"tags: {tagged_words}\")\n",
    "                    proper_nouns = []\n",
    "                    for sentence in tagged_words:\n",
    "                        for word, tag in sentence:\n",
    "                            if tag == 'NNP': # NNP denotes proper noun\n",
    "                                proper_nouns.append(word)\n",
    "                    print(proper_nouns)\n",
    "                    propNounsElse += len(set(proper_nouns))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    print(f\"Prop nouns fake {propNounsFake/fakeTotal} else: {propNounsElse/elseTotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueGraph(df):\n",
    "    fakeArticles = []\n",
    "    reliableArticles = []\n",
    "\n",
    "    fakeWords = []\n",
    "    reliableWords = []\n",
    "\n",
    "    #for y in df[\"content\"]:\n",
    "    for x in range(0, len(df)):\n",
    "        if df[\"type\"][x].lower() == \"fake\":\n",
    "            fakeWords = Counter(sorted(word_tokenize(df[\"content\"][x])))\n",
    "            fakeArticles.append(len(fakeWords))\n",
    "        else:\n",
    "            reliableWords = Counter(sorted(word_tokenize(df[\"content\"][x])))\n",
    "            reliableArticles.append(len(reliableWords))\n",
    "\n",
    "    AvFake = sum(fakeArticles)/len(fakeArticles)\n",
    "    AvReliable = sum(reliableArticles)/len(reliableArticles)\n",
    "    dif = (AvFake-AvReliable)/AvFake*100\n",
    "\n",
    "    print(\"Unique words in fake articles: \" + str(AvFake))\n",
    "    print(\"Unique words in reliable articles: \" + str(AvReliable))\n",
    "    print(\"Difference: {} %\".format(math.floor(dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fakenessFromWord(df, word):\n",
    "\n",
    "    fakeWord = 0\n",
    "    reliableWord = 0\n",
    "\n",
    "    fakeNoWord = 0\n",
    "    reliableNoWord = 0\n",
    "\n",
    "    word = word.lower()\n",
    "\n",
    "\n",
    "    for x in range(0, len(df)):\n",
    "        result = re.search(rf\"\\b{word}\\b\", str(df[\"content\"][x]))\n",
    "        try:\n",
    "            if not(result == None): #the word is found\n",
    "                if df[\"type\"][x].lower() == \"fake\":\n",
    "                    fakeWord += 1\n",
    "                else:\n",
    "                    reliableWord += 1\n",
    "            else:\n",
    "                if df[\"type\"][x].lower() == \"fake\":\n",
    "                    fakeNoWord += 1\n",
    "                else:\n",
    "                    reliableNoWord += 1\n",
    "        except:\n",
    "            pass\n",
    "    print(\"fakeword: {}\\n reliableword: {}\\n fakenoword: {} \\n reliableNoword: {}\".format(fakeWord, reliableWord, fakeNoWord, reliableNoWord))\n",
    "    print(df.shape)\n",
    "    #percentage of fake articles with the word out of all fake articles\n",
    "    preFake = (1 - (fakeNoWord/(fakeNoWord + fakeWord)))*100\n",
    "    print(\"Percentage of fake articles with the word: {}%\".format(preFake))\n",
    "\n",
    "    #percentage of reliable articles with the word out of all reliabel articles\n",
    "    preReliable = (1 - (reliableNoWord/(reliableNoWord + reliableWord)))*100\n",
    "    print(\"Percentage of reliable articles with the word: {}%\".format(preReliable))\n",
    "\n",
    "    #out of all articles with the word X% of them are fake\n",
    "    fakeWordCorrelation = (fakeWord/(fakeWord + reliableWord))*100\n",
    "    print(fakeWordCorrelation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclamationFunction(df):\n",
    "\n",
    "    fakeExclamations = []\n",
    "    nonFakeExclamations = []\n",
    "\n",
    "    fakeNoExclamations = 0\n",
    "    nonFakeNoExclamations = 0\n",
    "    k = 0\n",
    "\n",
    "    for x in range(0, len(df)):\n",
    "        if str(df['type'][x]).lower() == 'fake':\n",
    "            # counting number of fake articles\n",
    "            k += 1\n",
    "        excl = re.findall('!', df['content'][x])\n",
    "        if len(excl) == 0: \n",
    "            if str(df['type'][x]).lower() == 'fake':\n",
    "                fakeNoExclamations += 1\n",
    "            else:\n",
    "                nonFakeNoExclamations += 1\n",
    "        else:\n",
    "            if str(df['type'][x]).lower() == 'fake':\n",
    "                fakeExclamations.append(len(excl))\n",
    "            else:\n",
    "                nonFakeExclamations.append(len(excl))\n",
    "\n",
    "    # mean number of exclamation marks in fake and not fake articles\n",
    "    fakeExclMean = stats.mean(fakeExclamations)\n",
    "    nonFakeExclMean = stats.mean(nonFakeExclamations)\n",
    "\n",
    "    # percentage of fake articles without exclamations\n",
    "    fNE = (fakeNoExclamations / k)*100\n",
    "\n",
    "    print(\"If the article is fake and has exclamation marks, there are on average {} of them\".format(fakeExclMean))\n",
    "    print(\"If the article isn't fake, and has exclamation marks, there are on average {} of them\".format(nonFakeExclMean))\n",
    "\n",
    "    print(\"Of the {} total articles, {} of them are fake\".format(len(df), k))\n",
    "    print(\"Of the {} fake articles, {} don't have exclamation marks in - {}%\".format(k, fakeNoExclamations, fNE))\n",
    "\n",
    "    # if fake, percentage chance of exclamation\n",
    "    ifFake = ((k/len(df)) * (len(fakeExclamations)/k))/(\n",
    "                (k/len(df)) * (len(fakeExclamations)/k) + (\n",
    "                (1 - (k/len(df))) * (1 - (len(fakeExclamations)/k)))) * 100\n",
    "\n",
    "    # if not fake, perecentage chance of exclamation\n",
    "    ifNonFake = (((len(df) - k)/len(df)) * (len(nonFakeExclamations)/(\n",
    "                len(df) - k))) / ((((len(df) - k)/len(df)) * (\n",
    "                len(nonFakeExclamations)/(len(df) - k))) + ((1 - (\n",
    "                (len(df) - k)/len(df))) * (1 - (len(nonFakeExclamations)/(\n",
    "                len(df) - k))))) * 100\n",
    "\n",
    "    print(\"If an article is fake, there is a {}% chance that it has exclamation marks\".format(ifFake))\n",
    "    print(\"If an article isn't fake, there is a {}% chance that it has exclamation marks\".format(ifNonFake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing types to a binary classification\n",
    "# Reliable is either with type reliable, clickbait or political - becomes 1\n",
    "# Fake is the rest - becomes 0\n",
    "\n",
    "def classifierRelOrFake(df):\n",
    "    for x in range(0, len(df)):\n",
    "        if df['type'][x] == 'reliable' or df['type'][x] == 'clickbait' or df['type'][x] == 'political':\n",
    "            df['type'][x] = 1 \n",
    "        else:\n",
    "            df['type'][x] = 0 \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in range(len(df)):\n",
    "#     print(\"Authors:\",df['authors'][x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the article is fake and has exclamation marks, there are on average 3.114503816793893 of them\n",
      "If the article isn't fake, and has exclamation marks, there are on average 2.388888888888889 of them\n",
      "Of the 239 total articles, 146 of them are fake\n",
      "Of the 146 fake articles, 15 don't have exclamation marks in - 10.273972602739725%\n",
      "If an article is fake, there is a 93.20208566833975% chance that it has exclamation marks\n",
      "If an article isn't fake, there is a 13.260456273764259% chance that it has exclamation marks\n"
     ]
    }
   ],
   "source": [
    "# for x in range(0,len(df)):\n",
    "#     df.iloc[x] = zipfsLaw.zipfsFiltering(df.iloc[x])\n",
    "#     df.iloc[x] = removeStopwords(df.iloc[x])\n",
    "#     df.iloc[x] = keywordFiltering(df.iloc[x])\n",
    "#     df.iloc[x] = stemming.applyStemming(df.iloc[x])\n",
    "\n",
    "    #tilføj funktioner husk kun at give en linje\n",
    "# fakenessFromWord(df, \"trump\")\n",
    "exclamationFunction(df)\n",
    "classifierRelOrFake(df)\n",
    "# uniqueGraph(df)\n",
    "\n",
    "# exploringData(df)\n",
    "# print(df)\n",
    "df.to_csv(\"Results.csv\")\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['content'], df['type'],\n",
    "                                                    test_size=0.2,random_state=0)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, train_size=0.5,\n",
    "                                                random_state=0)\n",
    "# lr = LinearRegression()\n",
    "# model = lr.fit(X_train, y_train)\n",
    "# print(model.score(X_train, y_train))\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
